# üè≠ Azure Data Factory End -To-End

## üìå Descripci√≥n
Este proyecto tuvo como objetivo fortalecer mis habilidades en Azure Data Factory, realizando la migraci√≥n de tablas desde SQL Server mediante un **Integration Runtime Self-Hosted**.
Se implement√≥ un sistema de alertas autom√°ticas por correo electr√≥nico utilizando **Logic Apps** para notificar fallos en los pipelines.
Adem√°s, se aplic√≥ la **arquitectura Medallion** (Layers Bronze, Silver y Gold) para estructurar y optimizar el flujo de datos, garantizando una mayor calidad y trazabilidad en el proceso de transformaci√≥n.

## üéØ Objetivos

- Migrar datos de SQL Server a Azure con Integration Runtime.

- Aplicar arquitectura Medallion para optimizar datos.

- Configurar alertas autom√°ticas con Logic Apps.

- Usar Azure Key Vault para mayor seguridad en credenciales.

## üöÄ Tecnolog√≠as Utilizadas
üõ¢  SQL Server

üè≠ Azure Data Factory

üêô GitHub

üîë Azure Key Vault

## üíª Desarrollo
Este proyecto, desarrollado en Azure Data Factory, consisti√≥ en la migraci√≥n de tablas desde SQL Server y la extracci√≥n de datos en formato JSON desde GitHub.
Se implement√≥ Azure Key Vault para reforzar la seguridad en el manejo de credenciales y se utiliz√≥ Azure Data Lake Gen2 para almacenar la informaci√≥n en las capas Bronze, Silver y Gold, aplicando control de versiones mediante GitHub.

<p align="center">
  <img src="../image/arquitecturadraw.png" alt="Diagrama de Arquitectura">
</p>

## üìí Grupo de recurso
Desde el portal de Azure se creamos el grupo de recursos, dentro del cual se implementaron de manera individual los servicios necesarios para el proyecto: Azure Key Vault, Azure Data Lake Gen2, Azure SQL Database, Azure Data Factory y Logic Apps.

<p align="center">
  <img src="../image/GRUPO_RECURSO.png" alt="Diagrama de Arquitectura">
</p>

## üõ†Ô∏è Integration Runtime
En esta ocasi√≥n, como el objetivo es obtener las tablas desde SQL Server, se utiliz√≥ un Integration Runtime Self-Hosted para la configuraci√≥n de la conexi√≥n, permitiendo migrar datos desde entornos on-premise hacia la nube.

<p align="center">
  <img src="../image/INTEGRATIONRUNTIME.png" alt="Diagrama de Arquitectura">
</p>

## üîó Linked Service
En este proyecto, el uso de Linked Services permite establecer conexiones con recursos externos desde los cuales queremos obtener.
En este caso, se emple√≥ Azure Key Vault para gestionar de forma segura las credenciales utilizadas por los propios Linked Services, adem√°s de configurar conexiones hacia Azure SQL Database, Azure Data Lake Gen2, Web y SQL Server.

<p align="center">
  <img src="../image/LINKED.png" alt="Diagrama de Arquitectura">
</p>

## üß™ Pipeline
El pipeline PL_MIGRACION_OP, mediante la actividad Lookup configurada con una consulta SQL, obtiene la lista de tablas que comparten el mismo schema por defecto (dbo).
Para hacer m√°s din√°mico el proceso de migraci√≥n, se utiliz√≥ una actividad ForEach que recorre y procesa cada una de las tablas provenientes de SQL Server.
<p align="center">
  <img src="../image/PL_MIGRACION_OP.png" alt="Diagrama de Arquitectura">
</p>

Dentro del ForEach, se utilizaron par√°metros para optimizar la conexi√≥n y transferencia de la informaci√≥n.
En este caso, se emple√≥ el mismo nombre tanto en el source como en el sink, y los datos resultantes se almacenaron en Azure Data Lake Gen2 en formato Parquet.

<p align="center">
  <img src="../image/PL_COPY_OP.png" alt="Diagrama de Arquitectura">
</p>

Se obtuvo el archivo DimAirport.json desde GitHub mediante la actividad Web de Azure Data Factory.
Para ello, se utiliz√≥ la URL del archivo de la opci√≥n raw, lo permiti√≥ acceder directamente a su contenido y procesarlo dentro del pipeline.

<p align="center">
  <img src="../image/GITHUB.png" alt="Diagrama de Arquitectura">
</p>

Finalmente, este archivo JSON se envi√≥ a nuestro Data Lake Gen2, dentro de la carpeta Bronze y la subcarpeta GitHub, con el fin de mantener una mejor organizaci√≥n y mapeo de los datos.

<p align="center">
  <img src="../image/COPY_API.png" alt="Diagrama de Arquitectura">
</p>

En el pipeline PL_SQLDATALAKE se aplic√≥ una t√©cnica moderna de carga incremental, utilizando dos actividades Lookup.
En la primera, se defini√≥ una fecha inicial por 1900-01-01, y en la segunda, se obtuvo la fecha m√°xima disponible mediante una consulta SQL.
Esta configuraci√≥n permiti√≥ realizar la carga incremental de manera din√°mica y eficiente.

<p align="center">
  <img src="../image/PL_SQL.png" alt="Diagrama de Arquitectura">
</p>

Dentro de la actividad Copy Data, se ejecuta una consulta SQL filtrando los registros cuya fecha sea mayor a la de LastLoad y menor a la de LatestLoad.
Este enfoque permite obtener √∫nicamente las filas nuevas.
Posteriormente, en la actividad Watermark, se actualiza nuevamente la fecha de referencia con la √∫ltima obtenida, de manera que cada vez que ingresen nuevos datos, las fechas iniciales se actualicen constantemente.

<p align="center">
  <img src="../image/QUERY_NEW.png" alt="Diagrama de Arquitectura">
</p>

Mediante el uso de Execute Pipeline, se logr√≥ orquestar la ejecuci√≥n a nivel de pipelines, lo que permiti√≥ un mejor control y manejo del flujo de procesos.
Adem√°s, se configur√≥ una alerta utilizando la actividad Web, la cual se conecta con Logic Apps para notificar cualquier incidencia en la ejecuci√≥n.

<p align="center">
  <img src="../image/PL_PRINCIPAL.png" alt="Diagrama de Arquitectura">
</p>

## üíª Logic App

Dentro de Logic App, se defini√≥ el JSON en la secci√≥n Request y, posteriormente, en la actividad Email se configur√≥ el mensaje din√°mico que se enviar√°.
Al finalizar, se obtuvo la URL del flujo, la cual fue utilizada en la actividad Web del pipeline para establecer la conexi√≥n y enviar las alertas.
<p align="center">
  <img src="../image/LOGIC_APP.png" alt="Diagrama de Arquitectura">
</p>

## ‚õèÔ∏è Data Flows
Dentro de nuestro de DataTransform, se define la fuente de datos y se aplica las transformaciones necesarias, incluyendo la limpieza y modificaci√≥n de los datos para asegurar su calidad. Finalmente, los datos transformados se almacenan en formato Delta dentro de nuestro Data Lake, espec√≠ficamente en la subcarpeta denominada silver.

<p align="center">
  <img src="../image/SILVER.png" alt="Diagrama de Arquitectura">
</p>

Finalmente, en nuestro proyecto aplicamos operaciones de join para combinar tablas y as√≠ obtener informaci√≥n empresarial relevante para la toma de decisiones. En particular, obtuvimos el top 5 de reservas por aerol√≠nea, lo que permite analizar el desempe√±o y comportamiento del mercado.

<p align="center">
  <img src="../image/GOLD.png" alt="Diagrama de Arquitectura">
</p>

## üêà‚Äç‚¨õ GitHub
Finalmente, se subimos el proyecto a nuestro repositorio en GitHub, donde puede encontrarse en la rama principal (master branch).

<p align="center">
  <img src="../image/GitHub.png" alt="Diagrama de Arquitectura">
</p>

## üìö Conclusiones

- Este proyecto me ayud√≥ a desarrollar habilidades en la automatizaci√≥n y orquestaci√≥n de pipelines con Azure Data Factory.

- Gracias a este proyecto, pude comprender mejor la importancia de la arquitectura Medallion para mejorar la calidad y organizaci√≥n de los datos.

- La implementaci√≥n de alertas autom√°ticas y el uso de key vault me permiti√≥ fortalecer mis conocimientos en seguridad y monitoreo en Azure.
