# ğŸ­ Azure Data Factory End -To-End

## ğŸ“Œ DescripciÃ³n
Este proyecto tuvo como objetivo fortalecer mis habilidades en Azure Data Factory, realizando la migraciÃ³n de tablas desde SQL Server mediante un **Integration Runtime Self-Hosted**.
Se implementÃ³ un sistema de alertas automÃ¡ticas por correo electrÃ³nico utilizando **Logic Apps** para notificar fallos en los pipelines.
AdemÃ¡s, se aplicÃ³ la **arquitectura Medallion** (Layers Bronze, Silver y Gold) para estructurar y optimizar el flujo de datos, garantizando una mayor calidad y trazabilidad en el proceso de transformaciÃ³n.

## ğŸ¯ Objetivos

- Migrar datos de SQL Server a Azure con Integration Runtime.

- Aplicar arquitectura Medallion para optimizar datos.

- Configurar alertas automÃ¡ticas con Logic Apps.

- Usar Azure Key Vault para mayor seguridad en credenciales.

## ğŸš€ TecnologÃ­as Utilizadas
ğŸ›¢  SQL Server

ğŸ­ Azure Data Factory

ğŸ™ GitHub

ğŸ”‘ Azure Key Vault

## ğŸ’» Desarrollo
Este proyecto, desarrollado en Azure Data Factory, consistiÃ³ en la migraciÃ³n de tablas desde SQL Server y la extracciÃ³n de datos en formato JSON desde GitHub.
Se implementÃ³ Azure Key Vault para reforzar la seguridad en el manejo de credenciales y se utilizÃ³ Azure Data Lake Gen2 para almacenar la informaciÃ³n en las capas Bronze, Silver y Gold, aplicando control de versiones mediante GitHub.

<p align="center">
  <img src="../image/arquitecturadraw.png" alt="Diagrama de Arquitectura">
</p>

## ğŸ“’ Grupo de recurso
Desde el portal de Azure se creamos el grupo de recursos, dentro del cual se implementaron de manera individual los servicios necesarios para el proyecto: Azure Key Vault, Azure Data Lake Gen2, Azure SQL Database, Azure Data Factory y Logic Apps.

<p align="center">
  <img src="../image/GRUPO_RECURSO.png" alt="Diagrama de Arquitectura">
</p>

## ğŸ› ï¸ Integration Runtime
En esta ocasiÃ³n, como el objetivo es obtener las tablas desde SQL Server, se utilizÃ³ un Integration Runtime Self-Hosted para la configuraciÃ³n de la conexiÃ³n, permitiendo migrar datos desde entornos on-premise hacia la nube.

<p align="center">
  <img src="../image/INTEGRATIONRUNTIME.png" alt="Diagrama de Arquitectura">
</p>

## ğŸ”— Linked Service
En este proyecto, el uso de Linked Services permite establecer conexiones con recursos externos desde los cuales queremos obtener.
En este caso, se empleÃ³ Azure Key Vault para gestionar de forma segura las credenciales utilizadas por los propios Linked Services, ademÃ¡s de configurar conexiones hacia Azure SQL Database, Azure Data Lake Gen2, Web y SQL Server.

<p align="center">
  <img src="../image/LINKED.png" alt="Diagrama de Arquitectura">
</p>

## ğŸ§ª Pipeline
El pipeline PL_MIGRACION_OP, mediante la actividad Lookup configurada con una consulta SQL, obtiene la lista de tablas que comparten el mismo schema por defecto (dbo).
Para hacer mÃ¡s dinÃ¡mico el proceso de migraciÃ³n, se utilizÃ³ una actividad ForEach que recorre y procesa cada una de las tablas provenientes de SQL Server.
<p align="center">
  <img src="../image/PL_MIGRACION_OP.png" alt="Diagrama de Arquitectura">
</p>

Dentro del ForEach, se utilizaron parÃ¡metros para optimizar la conexiÃ³n y transferencia de la informaciÃ³n.
En este caso, se empleÃ³ el mismo nombre tanto en el source como en el sink, y los datos resultantes se almacenaron en Azure Data Lake Gen2 en formato Parquet.

<p align="center">
  <img src="../image/PL_COPY_OP.png" alt="Diagrama de Arquitectura">
</p>

Se obtuvo el archivo DimAirport.json desde GitHub mediante la actividad Web de Azure Data Factory.
Para ello, se utilizÃ³ la URL del archivo de la opciÃ³n raw, lo permitiÃ³ acceder directamente a su contenido y procesarlo dentro del pipeline.

<p align="center">
  <img src="../image/GITHUB.png" alt="Diagrama de Arquitectura">
</p>

Finalmente, este archivo JSON se enviÃ³ a nuestro Data Lake Gen2, dentro de la carpeta Bronze y la subcarpeta GitHub, con el fin de mantener una mejor organizaciÃ³n y mapeo de los datos.

<p align="center">
  <img src="../image/COPY_API.png" alt="Diagrama de Arquitectura">
</p>

En el pipeline PL_SQLDATALAKE se aplicÃ³ una tÃ©cnica moderna de carga incremental, utilizando dos actividades Lookup.
En la primera, se definiÃ³ una fecha inicial por 1900-01-01, y en la segunda, se obtuvo la fecha mÃ¡xima disponible mediante una consulta SQL.
Esta configuraciÃ³n permitiÃ³ realizar la carga incremental de manera dinÃ¡mica y eficiente.

<p align="center">
  <img src="../image/PL_SQL.png" alt="Diagrama de Arquitectura">
</p>

Dentro de la actividad Copy Data, se ejecuta una consulta SQL filtrando los registros cuya fecha sea mayor a la de LastLoad y menor a la de LatestLoad.
Este enfoque permite obtener Ãºnicamente las filas nuevas.
Posteriormente, en la actividad Watermark, se actualiza nuevamente la fecha de referencia con la Ãºltima obtenida, de manera que cada vez que ingresen nuevos datos, las fechas iniciales se actualicen constantemente.

<p align="center">
  <img src="../image/QUERY_NEW.png" alt="Diagrama de Arquitectura">
</p>

Mediante el uso de Execute Pipeline, se logrÃ³ orquestar la ejecuciÃ³n a nivel de pipelines, lo que permitiÃ³ un mejor control y manejo del flujo de procesos.
AdemÃ¡s, se configurÃ³ una alerta utilizando la actividad Web, la cual se conecta con Logic Apps para notificar cualquier incidencia en la ejecuciÃ³n.

<p align="center">
  <img src="../image/PL_PRINCIPAL.png" alt="Diagrama de Arquitectura">
</p>

## ğŸ’» Logic App

Dentro de Logic App, se definiÃ³ el JSON en la secciÃ³n Request y, posteriormente, en la actividad Email se configurÃ³ el mensaje dinÃ¡mico que se enviarÃ¡.
Al finalizar, se obtuvo la URL del flujo, la cual fue utilizada en la actividad Web del pipeline para establecer la conexiÃ³n y enviar las alertas.
<p align="center">
  <img src="../image/LOGIC_APP.png" alt="Diagrama de Arquitectura">
</p>

## â›ï¸ Data Flows
Dentro de nuestro de DataTransform, se define la fuente de datos y se aplica las transformaciones necesarias, incluyendo la limpieza y modificaciÃ³n de los datos para asegurar su calidad. Finalmente, los datos transformados se almacenan en formato Delta dentro de nuestro Data Lake, especÃ­ficamente en la subcarpeta denominada silver.

<p align="center">
  <img src="../image/SILVER.png" alt="Diagrama de Arquitectura">
</p>

Finalmente, en nuestro proyecto aplicamos operaciones de join para combinar tablas y asÃ­ obtener informaciÃ³n empresarial relevante para la toma de decisiones. En particular, obtuvimos el top 5 de reservas por aerolÃ­nea, lo que permite analizar el desempeÃ±o y comportamiento del mercado.

<p align="center">
  <img src="../image/GOLD.png" alt="Diagrama de Arquitectura">
</p>

## ğŸˆâ€â¬› GitHub

## ğŸ“š Conclusion
